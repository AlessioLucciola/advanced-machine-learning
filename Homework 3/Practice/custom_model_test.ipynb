{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import h36motion3d as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.loss_funcs import *\n",
    "from utils.data_utils import define_actions\n",
    "from utils.h36_3d_viz import visualize\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#reproducibility stuff\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d_k = key.size(-1) #-1 refers to the last dimension\n",
    "        # Compute the dot product of the query and key, and scale it\n",
    "        #attn = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) #Note: Transpose(-2, -1) swaps the second last dimension with the last dimension\n",
    "        attn = torch.einsum('BHLD, BHMD -> BHLM', query, key) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(value, dim=-1))\n",
    "        \n",
    "        #output = torch.matmul(attn, value)\n",
    "        output = torch.einsum('BHLM, BHLD -> BHLD', attn, value)\n",
    "\n",
    "        return output, attn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
