{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import h36motion3d as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.loss_funcs import *\n",
    "from utils.data_utils import define_actions\n",
    "from utils.h36_3d_viz import visualize\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#reproducibility stuff\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d_k = key.size(-1) #-1 refers to the last dimension\n",
    "        # Compute the dot product of the query and key, and scale it\n",
    "        #attn = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) #Note: Transpose(-2, -1) swaps the second last dimension with the last dimension\n",
    "        attn = torch.einsum('BHLD, BHMD -> BHLM', query, key) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(value, dim=-1))\n",
    "        \n",
    "        #output = torch.matmul(attn, value)\n",
    "        output = torch.einsum('BHLM, BHLD -> BHLD', attn, value)\n",
    "\n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model) #Initialize the positional encoding max_len*d_model vector with zeros\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) #Initialize a max_len vector\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) #Compute the denominator of the pe fixed formula\n",
    "        pe[:, 0::2] = torch.sin(position*div_term) #sin if position index is even\n",
    "        pe[:, 1::2] = torch.cos(position*div_term) #cos if position index is odd\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] #Simply concatenate the input x with the positional encoding vectors\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''Class that defines a single block of the encoder transformer (see the structure on notes page 53)'''\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, slf_attn_mask=None):\n",
    "        #Note: Layer norm and residual connection are applied in the inner classes (MultiHeadAttention and PositionwiseFeedForward)\n",
    "        enc_output, enc_slf_atten = self.slf_attn(enc_input, enc_input, enc_input, mask=slf_attn_mask) #Apply the multi-head self-attention. Assume the input is an embedding with time signal.\n",
    "        enc_output = self.pos_ffn(enc_output) #Apply the feed-forward network\n",
    "        return enc_output, enc_slf_atten #Return the output of the two operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''Class that defines a single block of the encoder transformer (see the structure on notes page 54)'''\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        #Note: Layer norm and residual connection are applied in the inner classes (MultiHeadAttention and PositionwiseFeedForward)\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout) #First (Masked) Multi-head self-attention of the decoder\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout) #Second Multi-head self-attention that takes into consideration the output of the encoder\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, dec_input, enc_output, slf_attn_mask=None, dec_enc_attn_mask=None):\n",
    "         #Apply the (masked) multi-head self-attention. Here, k, v and q comes from the decoder (the already predicted units)\n",
    "        dec_output, dec_slf_attn = self.slf_attn(dec_input, dec_input, dec_input, mask=slf_attn_mask)\n",
    "        #Apply the (masked) multi-head self-attention. Here, k, v comes from the encoder (the memory - output of the encoder) while q comes from the decoder (the already predicted units)\n",
    "        dec_output, dec_enc_attn = self.enc_attn(dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n",
    "        dec_output = self.pos_ffn(dec_output) #Apply the feed forward network\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn #return the output of (of the 3) each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the MultiHeadAttention class like this:\n",
    "\\begin{split}\n",
    "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
    "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE!\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k # Dimension of the keys\n",
    "        self.d_v = d_v # Dimension of the values\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False) #Compute W_q (query weights)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False) #Computer W_k (key weights)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False) #Compute W_v (Value weights)\n",
    "        #self.fc = nn.Linear(n_head * d_v, d_model, bias=False) #MLP to be applied after layer norm\n",
    "\n",
    "        #Note on dimensionality: The dimensionality for both keys and queries is N*d_k, that's why we use the same variable for define the weights\n",
    "        #Concerning the values, is d_v and it defines the dimensionality of the final output\n",
    "\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        def forward(self, q, k, v, mask=None):\n",
    "            d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "            sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "            residual = q #Save the queries in order to compute the residual connection after the Multi-head Attention\n",
    "            \n",
    "            # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "            # Separate different heads: b x lq x n x dv (!!!)\n",
    "            q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "            k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "            v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "            # Transpose for attention dot product: b x n x lq x dv\n",
    "            q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "            q, attn = self.attention(q, k, v, mask=mask) #Apply the dot product operation\n",
    "            # Transpose to move the head dimension back: b x lq x n x dv\n",
    "            # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "            q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "            q = self.dropout(self.fc(q))\n",
    "            q += residual #Compute residual connection\n",
    "\n",
    "            q = self.layer_norm(q) #Apply layer norm\n",
    "\n",
    "            return q, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_in, d_hidden) # position-wise\n",
    "        self.w2 = nn.Linear(d_hidden, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x #We save the input before applying the feed-forward operation and after, we sum the residual\n",
    "\n",
    "        x = self.w2(F.relu(self.w1(x))) #Apply the feed-forward pass (w1 -> ReLU -> w2)\n",
    "        x = self.dropout(x)\n",
    "        x += residual #Compute residual connection\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    ''' For masking out the subsequent info. '''\n",
    "    if (len(seq.shape) == 4):\n",
    "      sz_b, c, len_s, dim = seq.size()\n",
    "    else:\n",
    "      sz_b, len_s, dim = seq.size()\n",
    "    subsequent_mask = (1 - torch.triu(\n",
    "        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' A encoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, pad_idx, dropout=0.1, n_position=200, scale_emb=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.position_enc = PositionalEncoding(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)]) #Put all Encoder layers in the stack\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        #self.scale_emb = scale_emb\n",
    "        #self.d_model = d_model\n",
    "\n",
    "    def forward(self, src_seq, src_mask, return_attns=False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        enc_output = self.dropout(self.position_enc(src_seq)) #Apply positional encoding to the input\n",
    "        enc_output = self.layer_norm(enc_output) #Apply layer norm\n",
    "\n",
    "        for enc_layer in self.layer_stack: #For each Encoder layer\n",
    "            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask) #Apply it\n",
    "            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return enc_output, enc_slf_attn_list\n",
    "        return enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    ''' A decoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, d_word_vec, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, pad_idx, n_position=200, dropout=0.1, scale_emb=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.position_enc = PositionalEncoding(d_word_vec)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)]) #Put all Encoder layers in the stack\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, trg_seq, trg_mask, enc_output, src_mask, inference):\n",
    "        if not inference: #If you are training the model..\n",
    "          dec_output = self.dropout(self.position_enc(trg_seq))\n",
    "          dec_output = self.layer_norm(dec_output)\n",
    "\n",
    "          for dec_layer in self.layer_stack: #For each Decoder layer\n",
    "              dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                  dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask) #Apply it\n",
    "          return dec_output\n",
    "\n",
    "        else: #If you are testing the model (during inference)\n",
    "          for i in range(trg_seq.shape[1]): \n",
    "            #print(trg_seq[0, :, 3])\n",
    "            if i!= trg_seq.shape[1]-1:\n",
    "              dec_output = self.dropout(self.position_enc(trg_seq))\n",
    "              dec_output = self.layer_norm(dec_output)\n",
    "\n",
    "              for dec_layer in self.layer_stack:\n",
    "                  dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                      dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n",
    "              trg_seq[:, i+1, :] = dec_output[:, -1, :]\n",
    "            else:\n",
    "              dec_output = self.dropout(self.position_enc(trg_seq))\n",
    "              dec_output = self.layer_norm(dec_output)\n",
    "\n",
    "              for dec_layer in self.layer_stack:\n",
    "                  dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                      dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n",
    "\n",
    "          return torch.cat((trg_seq[:, 1:, :], dec_output[:, -1, :].unsqueeze(1)), dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
