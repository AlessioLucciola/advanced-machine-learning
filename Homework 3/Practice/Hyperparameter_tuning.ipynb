{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/YOURPATH/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import h36motion3d as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.loss_funcs import *\n",
    "from utils.data_utils import define_actions\n",
    "from utils.h36_3d_viz import visualize\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise stick with cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device,  '- Type:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Arguments to setup the datasets\n",
    "datas = 'h36m' # dataset name\n",
    "path = './data/h3.6m/h3.6m/dataset'\n",
    "input_n=10 # number of frames to train on (default=10)\n",
    "output_n=25 # number of frames to predict on\n",
    "input_dim=3 # dimensions of the input coordinates(default=3)\n",
    "skip_rate=1 # # skip rate of frames\n",
    "joints_to_consider=22\n",
    "\n",
    "\n",
    "#FLAGS FOR THE TRAINING\n",
    "mode='train' #choose either train or test mode\n",
    "\n",
    "batch_size_test=8\n",
    "model_path= './checkpoints/' # path to the model checkpoint file\n",
    "\n",
    "actions_to_consider_test='all' # actions to test on.\n",
    "model_name = datas+'_3d_'+str(output_n)+'frames_ckpt' #the model name to save/load\n",
    "\n",
    "#FLAGS FOR THE VISUALIZATION\n",
    "actions_to_consider_viz='all' # actions to visualize\n",
    "visualize_from='test'\n",
    "n_viz=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\OneDrive\\Desktop\\Projects\\advanced-machine-learning\\Homework 3\\Practice\\utils\\h36motion3d.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  subs = np.array([[1, 6, 7, 8, 9], [11], [5]])  # , 6, 7, 8, 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Validation Dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "print('Loading Train Dataset...')\n",
    "dataset = datasets.Datasets(path,input_n,output_n,skip_rate, split=0)\n",
    "print('Loading Validation Dataset...')\n",
    "vald_dataset = datasets.Datasets(path,input_n,output_n,skip_rate, split=1)\n",
    "\n",
    "#! Note: Ignore warning:  \"VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training dataset length: 180077\n",
      ">>> Validation dataset length: 28110\n"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "\n",
    "print('>>> Training dataset length: {:d}'.format(dataset.__len__()))\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)#\n",
    "\n",
    "print('>>> Validation dataset length: {:d}'.format(vald_dataset.__len__()))\n",
    "vald_loader = DataLoader(vald_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from models.sttr.sttformer import Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s'%device)\n",
    "\n",
    "n_heads = 1\n",
    "\n",
    "def instantiate_model(num_joints=joints_to_consider, num_frames=input_n, num_frames_out=output_n, num_heads=n_heads, num_channels=3, kernel_size=[3,3], use_pes=True):\n",
    "    model = Model(num_joints, num_frames, num_frames_out, num_heads, num_channels, kernel_size, use_pes).to(device)\n",
    "    print('total number of parameters of the network is: '+str(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_grad=None # select max norm to clip gradients\n",
    "# Argument for training\n",
    "n_epochs=41\n",
    "log_step = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "def train(model, data_loader, vald_loader, lr, ml, wd, gamma, use_scheduler=True, save_and_plot=True, path_to_save_model=None, path_to_save_plots=None):\n",
    "    optimizer=optim.Adam(model.parameters(),lr=lr,weight_decay=wd)\n",
    "\n",
    "    if use_scheduler:\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=ml, gamma=gamma)\n",
    "\n",
    "    # Creation of a folder where to save plots\n",
    "    if path.exists(path_to_save_plots) == False:\n",
    "        os.mkdir(path_to_save_plots)\n",
    "\n",
    "    # Creation of a folder where to save models\n",
    "    if path.exists(path_to_save_model) == False:\n",
    "        os.mkdir(path_to_save_model)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_loss_best = 1000\n",
    "\n",
    "    dim_used = np.array([6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25,\n",
    "                        26, 27, 28, 29, 30, 31, 32, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
    "                        46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 68,\n",
    "                        75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92])\n",
    "\n",
    "    for epoch in range(n_epochs-1):\n",
    "        running_loss=0\n",
    "        n=0\n",
    "        model.train()\n",
    "        for cnt,batch in enumerate(data_loader):\n",
    "            batch=batch.float().to(device)\n",
    "            batch_dim=batch.shape[0]\n",
    "            n+=batch_dim\n",
    "\n",
    "            sequences_train=batch[:, 0:input_n, dim_used].view(-1,input_n,len(dim_used)//3,3).permute(0,3,1,2)\n",
    "            sequences_gt=batch[:, input_n:input_n+output_n, dim_used].view(-1,output_n,len(dim_used)//3,3)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            sequences_predict=model(sequences_train).view(-1, output_n, joints_to_consider, 3)\n",
    "\n",
    "\n",
    "            loss=mpjpe_error(sequences_predict,sequences_gt)\n",
    "\n",
    "\n",
    "            if cnt % log_step == 0:\n",
    "                print('[Epoch: %d, Iteration: %5d]  training loss: %.3f' %(epoch + 1, cnt + 1, loss.item()))\n",
    "\n",
    "            loss.backward()\n",
    "            if clip_grad is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),clip_grad)\n",
    "\n",
    "            optimizer.step()\n",
    "            running_loss += loss*batch_dim\n",
    "\n",
    "        train_loss.append(running_loss.detach().cpu()/n)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_loss=0\n",
    "            n=0\n",
    "            for cnt,batch in enumerate(vald_loader):\n",
    "                batch=batch.float().to(device)\n",
    "                batch_dim=batch.shape[0]\n",
    "                n+=batch_dim\n",
    "\n",
    "\n",
    "                sequences_train=batch[:, 0:input_n, dim_used].view(-1,input_n,len(dim_used)//3,3).permute(0,3,1,2)\n",
    "                sequences_gt=batch[:, input_n:input_n+output_n, dim_used].view(-1,output_n,len(dim_used)//3,3)\n",
    "\n",
    "                sequences_predict=model(sequences_train).view(-1, output_n, joints_to_consider, 3)\n",
    "                loss=mpjpe_error(sequences_predict,sequences_gt)\n",
    "\n",
    "                if cnt % log_step == 0:\n",
    "                            print('[Epoch: %d, Iteration: %5d]  validation loss: %.3f' %(epoch + 1, cnt + 1, loss.item()))\n",
    "                running_loss+=loss*batch_dim\n",
    "            val_loss.append(running_loss.detach().cpu()/n)\n",
    "            if running_loss/n < val_loss_best:\n",
    "                val_loss_best = running_loss/n\n",
    "\n",
    "        if use_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # save and plot model every 5 epochs\n",
    "        '''\n",
    "        Insert your code below. Use the argument path_to_save_model to save the model to the path specified.\n",
    "        '''\n",
    "        if save_and_plot and (epoch+1)%5==0 and epoch!=0: # Save the model and display the losses every 5 epochs\n",
    "            torch.save(model.state_dict(), f'{path_to_save_model}/h36m_3d_25frames_ckpt_epoch_{epoch+1}_q1.pt')\n",
    "\n",
    "            # Plot the training and validation loss\n",
    "            fig, ax = plt.subplots()\n",
    "            x_tick_freq = 1 if epoch < 16 else 2 if epoch<26 else 4\n",
    "            epochs = range(1, epoch+2, x_tick_freq) # The epoch numbering in the plots starts from 1 in order to stick with the code above\n",
    "            x_list = list(range(1, len(train_loss)+1))\n",
    "            ax.plot(x_list, train_loss, 'r', label='Train loss') # Line that displays the train loss\n",
    "            ax.plot(x_list, val_loss, 'g', label='Val loss') # Line that displays the validation loss\n",
    "            params = \"Hyperparams: lr-> \" + str(lr) + \", ml->\" + str(ml) + \", wd->\" + str(wd)\n",
    "            ax.set_xticks(list(epochs))\n",
    "            ax.set_xlabel('Epochs \\n ' + params)\n",
    "            ax.set_ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.title('Loss History', fontsize=12)\n",
    "\n",
    "            plt.savefig(path_to_save_plots + \"/loss_epoch_\"+str(epoch+1)+\"-\"+str(lr)+\"-\"+str(ml)+\"-\"+str(wd)+\".png\", bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of parameters of the network is: 26859\n",
      "[Epoch: 1, Iteration:     1]  training loss: 558.251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aless\\OneDrive\\Desktop\\Projects\\advanced-machine-learning\\Homework 3\\Practice\\Hyperparameter_tuning.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/OneDrive/Desktop/Projects/advanced-machine-learning/Homework%203/Practice/Hyperparameter_tuning.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m lr, ml, wd \u001b[39min\u001b[39;00m tqdm(all_hp_combinations):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/OneDrive/Desktop/Projects/advanced-machine-learning/Homework%203/Practice/Hyperparameter_tuning.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     model \u001b[39m=\u001b[39m instantiate_model() \u001b[39m# All default model parameters are chosen\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aless/OneDrive/Desktop/Projects/advanced-machine-learning/Homework%203/Practice/Hyperparameter_tuning.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     training_losses, validation_losses \u001b[39m=\u001b[39m train(model, data_loader, vald_loader, lr, ml, wd, gamma\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, use_scheduler\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, save_and_plot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, path_to_save_model\u001b[39m=\u001b[39;49mpath_to_save_model, path_to_save_plots\u001b[39m=\u001b[39;49mpath_to_save_plots)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/OneDrive/Desktop/Projects/advanced-machine-learning/Homework%203/Practice/Hyperparameter_tuning.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# Calculate the minimum validation loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/OneDrive/Desktop/Projects/advanced-machine-learning/Homework%203/Practice/Hyperparameter_tuning.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     min_validation_loss \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(validation_losses)\n",
      "\u001b[1;32mc:\\Users\\aless\\OneDrive\\Desktop\\Projects\\advanced-machine-learning\\Homework 3\\Practice\\Hyperparameter_tuning.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/OneDrive/Desktop/Projects/advanced-machine-learning/Homework%203/Practice/Hyperparameter_tuning.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mif\u001b[39;00m cnt \u001b[39m%\u001b[39m log_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/OneDrive/Desktop/Projects/advanced-machine-learning/Homework%203/Practice/Hyperparameter_tuning.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[Epoch: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, Iteration: \u001b[39m\u001b[39m%5d\u001b[39;00m\u001b[39m]  training loss: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, cnt \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, loss\u001b[39m.\u001b[39mitem()))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aless/OneDrive/Desktop/Projects/advanced-machine-learning/Homework%203/Practice/Hyperparameter_tuning.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/OneDrive/Desktop/Projects/advanced-machine-learning/Homework%203/Practice/Hyperparameter_tuning.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mif\u001b[39;00m clip_grad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/OneDrive/Desktop/Projects/advanced-machine-learning/Homework%203/Practice/Hyperparameter_tuning.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(),clip_grad)\n",
      "File \u001b[1;32mc:\\Users\\aless\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aless\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-1, 1e-2]\n",
    "milestones = [[10, 25, 30], [15, 25, 35], [15, 30]]\n",
    "weight_decays = [0, 1e-2, 1e-5]\n",
    "\n",
    "all_hp_combinations = [(lr, ml, wd) for lr in learning_rates for ml in milestones for wd in weight_decays]\n",
    "\n",
    "best_validation_loss = float('inf')\n",
    "best_hyperparameters = None\n",
    "\n",
    "path_to_save_model = './HyperparamsTuning/models'\n",
    "path_to_save_plots = './HyperparamsTuning/plots'\n",
    "\n",
    "# Creation of a folder where to save plots\n",
    "if path.exists(\"./HyperparamsTuning/\") == False:\n",
    "    os.mkdir(\"./HyperparamsTuning/\")\n",
    "\n",
    "for lr, ml, wd in tqdm(all_hp_combinations):\n",
    "    model = instantiate_model() # All default model parameters are chosen\n",
    "    training_losses, validation_losses = train(model, data_loader, vald_loader, lr, ml, wd, gamma=0.1, use_scheduler=True, save_and_plot=True, path_to_save_model=path_to_save_model, path_to_save_plots=path_to_save_plots)\n",
    "\n",
    "    # Calculate the minimum validation loss\n",
    "    min_validation_loss = min(validation_losses)\n",
    "\n",
    "    if min_validation_loss < best_validation_loss:\n",
    "        best_validation_loss = min_validation_loss\n",
    "        best_hyperparameters = (lr, ml, wd)\n",
    "\n",
    "print(f\"Best validation loss reached: {best_validation_loss}\")\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
